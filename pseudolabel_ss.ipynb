{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "UNLABELED_BS = 256\n",
    "TRAIN_BS = 32\n",
    "TEST_BS = 1024\n",
    "\n",
    "num_train_samples = 1000\n",
    "samples_per_class = int(num_train_samples/10)\n",
    "\n",
    "x = pd.read_csv('data/mnist_train.csv')\n",
    "y = x['label']\n",
    "x.drop(['label'], inplace = True, axis = 1)\n",
    "\n",
    "x_test = pd.read_csv('data/mnist_test.csv')\n",
    "y_test = x_test['label']\n",
    "x_test.drop(['label'], inplace = True, axis = 1)\n",
    "\n",
    "x_train, x_unlabeled = x[y.values == 0].values[:samples_per_class], x[y.values == 0].values[samples_per_class:]\n",
    "y_train = y[y.values == 0].values[:samples_per_class]\n",
    "\n",
    "for i in range(1,10):\n",
    "    x_train = np.concatenate([x_train, x[y.values == i].values[:samples_per_class]], axis = 0)\n",
    "    y_train = np.concatenate([y_train, y[y.values == i].values[:samples_per_class]], axis = 0)\n",
    "    \n",
    "    x_unlabeled = np.concatenate([x_unlabeled, x[y.values == i].values[samples_per_class:]], axis = 0)\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "x_train = normalizer.fit_transform(x_train)\n",
    "x_unlabeled = normalizer.transform(x_unlabeled)\n",
    "x_test = normalizer.transform(x_test.values)\n",
    "# x_train = x_train / 255.0\n",
    "# x_test = x_test.values / 255.0\n",
    "\n",
    "\n",
    "x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor) \n",
    "\n",
    "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test.values).type(torch.LongTensor)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test = torch.utils.data.TensorDataset(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananyak/.local/lib/python3.6/site-packages/ipykernel_launcher.py:53: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50489d3d283b4b32a52961ba79410188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 : Train Loss : 0.00737 | Test Acc : 9.58000 | Test Loss : 2.303 \n",
      "Epoch: 10 : Train Loss : 0.00737 | Test Acc : 9.58000 | Test Loss : 2.302 \n",
      "Epoch: 20 : Train Loss : 0.00734 | Test Acc : 25.61000 | Test Loss : 2.290 \n",
      "Epoch: 30 : Train Loss : 0.00340 | Test Acc : 77.25000 | Test Loss : 0.735 \n",
      "Epoch: 40 : Train Loss : 0.00148 | Test Acc : 89.66000 | Test Loss : 0.334 \n",
      "Epoch: 50 : Train Loss : 0.00096 | Test Acc : 89.48000 | Test Loss : 0.326 \n",
      "Epoch: 60 : Train Loss : 0.00069 | Test Acc : 92.75000 | Test Loss : 0.228 \n",
      "Epoch: 70 : Train Loss : 0.00054 | Test Acc : 93.60000 | Test Loss : 0.202 \n",
      "Epoch: 80 : Train Loss : 0.00040 | Test Acc : 93.72000 | Test Loss : 0.196 \n",
      "Epoch: 90 : Train Loss : 0.00031 | Test Acc : 94.37000 | Test Loss : 0.187 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = TRAIN_BS, shuffle = True, num_workers = 8)\n",
    "\n",
    "unlabeled_train = torch.from_numpy(x_unlabeled).type(torch.FloatTensor)\n",
    "\n",
    "unlabeled = torch.utils.data.TensorDataset(unlabeled_train)\n",
    "unlabeled_loader = torch.utils.data.DataLoader(unlabeled, batch_size = UNLABELED_BS, shuffle = True, num_workers = 8)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = TEST_BS, shuffle = True, num_workers = 8)\n",
    "\n",
    "# Architecture from : https://github.com/peimengsui/semi_supervised_mnist\n",
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "            self.conv2 = nn.Conv2d(20, 40, kernel_size=5)\n",
    "            self.conv2_drop = nn.Dropout2d()\n",
    "            self.fc1 = nn.Linear(640, 150)\n",
    "            self.fc2 = nn.Linear(150, 10)\n",
    "            self.log_softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1,1,28,28)\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "            x = x.view(-1, 640)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.log_softmax(x)\n",
    "            return x\n",
    "        \n",
    "net = Net()\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0 \n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data\n",
    "            output = model(data)\n",
    "            predicted = torch.max(output,1)[1]\n",
    "            correct += (predicted == labels).sum()\n",
    "            loss += F.nll_loss(output, labels).item()\n",
    "\n",
    "    return (float(correct)/len(test)) *100, (loss/len(test_loader))\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "def train_supervised(model, train_loader, test_loader):\n",
    "    optimizer = torch.optim.SGD( model.parameters(), lr = 0.1)\n",
    "    EPOCHS = 100\n",
    "    model.train()\n",
    "    for epoch in tqdm_notebook(range(EPOCHS)):\n",
    "        correct = 0\n",
    "        running_loss = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch, y_batch\n",
    "            \n",
    "            output = model(X_batch)\n",
    "            labeled_loss = F.nll_loss(output, y_batch)\n",
    "                       \n",
    "            optimizer.zero_grad()\n",
    "            labeled_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += labeled_loss.item()\n",
    "        \n",
    "        if epoch %10 == 0:\n",
    "            test_acc, test_loss = evaluate(model, test_loader)\n",
    "            print('Epoch: {} : Train Loss : {:.5f} | Test Acc : {:.5f} | Test Loss : {:.3f} '.format(epoch, running_loss/(10 * len(train)), test_acc, test_loss))\n",
    "            model.train()\n",
    "\n",
    "train_supervised(net, train_loader, test_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc : 94.33000 | Test Loss : 0.194 \n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = evaluate(net, test_loader)\n",
    "print('Test Acc : {:.5f} | Test Loss : {:.3f} '.format(test_acc, test_loss))\n",
    "torch.save(net.state_dict(), 'saved_models/supervised_weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananyak/.local/lib/python3.6/site-packages/ipykernel_launcher.py:35: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86317abfdcb44a34a25387b84b9f34e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 : Alpha Weight : 0.02500 | Test Acc : 94.22000 | Test Loss : 0.200 \n",
      "Epoch: 1 : Alpha Weight : 0.05000 | Test Acc : 94.76000 | Test Loss : 0.191 \n",
      "Epoch: 2 : Alpha Weight : 0.07500 | Test Acc : 94.86000 | Test Loss : 0.187 \n",
      "Epoch: 3 : Alpha Weight : 0.10000 | Test Acc : 94.78000 | Test Loss : 0.189 \n",
      "Epoch: 4 : Alpha Weight : 0.12500 | Test Acc : 94.77000 | Test Loss : 0.186 \n",
      "Epoch: 5 : Alpha Weight : 0.15000 | Test Acc : 94.92000 | Test Loss : 0.186 \n",
      "Epoch: 6 : Alpha Weight : 0.17500 | Test Acc : 95.08000 | Test Loss : 0.177 \n",
      "Epoch: 7 : Alpha Weight : 0.20000 | Test Acc : 95.26000 | Test Loss : 0.176 \n",
      "Epoch: 8 : Alpha Weight : 0.22500 | Test Acc : 95.27000 | Test Loss : 0.176 \n",
      "Epoch: 9 : Alpha Weight : 0.25000 | Test Acc : 95.29000 | Test Loss : 0.172 \n",
      "Epoch: 10 : Alpha Weight : 0.27500 | Test Acc : 95.51000 | Test Loss : 0.167 \n",
      "Epoch: 11 : Alpha Weight : 0.30000 | Test Acc : 95.60000 | Test Loss : 0.161 \n",
      "Epoch: 12 : Alpha Weight : 0.32500 | Test Acc : 95.60000 | Test Loss : 0.164 \n",
      "Epoch: 13 : Alpha Weight : 0.35000 | Test Acc : 95.55000 | Test Loss : 0.166 \n",
      "Epoch: 14 : Alpha Weight : 0.37500 | Test Acc : 95.76000 | Test Loss : 0.159 \n",
      "Epoch: 15 : Alpha Weight : 0.40000 | Test Acc : 95.67000 | Test Loss : 0.157 \n",
      "Epoch: 16 : Alpha Weight : 0.42500 | Test Acc : 95.88000 | Test Loss : 0.154 \n",
      "Epoch: 17 : Alpha Weight : 0.45000 | Test Acc : 95.93000 | Test Loss : 0.147 \n",
      "Epoch: 18 : Alpha Weight : 0.47500 | Test Acc : 95.97000 | Test Loss : 0.147 \n",
      "Epoch: 19 : Alpha Weight : 0.50000 | Test Acc : 96.05000 | Test Loss : 0.148 \n",
      "Epoch: 20 : Alpha Weight : 0.52500 | Test Acc : 96.09000 | Test Loss : 0.149 \n",
      "Epoch: 21 : Alpha Weight : 0.55000 | Test Acc : 96.14000 | Test Loss : 0.139 \n",
      "Epoch: 22 : Alpha Weight : 0.57500 | Test Acc : 96.16000 | Test Loss : 0.142 \n",
      "Epoch: 23 : Alpha Weight : 0.60000 | Test Acc : 96.22000 | Test Loss : 0.134 \n",
      "Epoch: 24 : Alpha Weight : 0.62500 | Test Acc : 96.30000 | Test Loss : 0.140 \n",
      "Epoch: 25 : Alpha Weight : 0.65000 | Test Acc : 96.45000 | Test Loss : 0.135 \n",
      "Epoch: 26 : Alpha Weight : 0.67500 | Test Acc : 96.46000 | Test Loss : 0.134 \n",
      "Epoch: 27 : Alpha Weight : 0.70000 | Test Acc : 96.49000 | Test Loss : 0.133 \n",
      "Epoch: 28 : Alpha Weight : 0.72500 | Test Acc : 96.42000 | Test Loss : 0.128 \n",
      "Epoch: 29 : Alpha Weight : 0.75000 | Test Acc : 96.54000 | Test Loss : 0.124 \n",
      "Epoch: 30 : Alpha Weight : 0.77500 | Test Acc : 96.60000 | Test Loss : 0.125 \n",
      "Epoch: 31 : Alpha Weight : 0.80000 | Test Acc : 96.59000 | Test Loss : 0.126 \n",
      "Epoch: 32 : Alpha Weight : 0.82500 | Test Acc : 96.79000 | Test Loss : 0.125 \n",
      "Epoch: 33 : Alpha Weight : 0.85000 | Test Acc : 96.85000 | Test Loss : 0.122 \n",
      "Epoch: 34 : Alpha Weight : 0.87500 | Test Acc : 96.81000 | Test Loss : 0.121 \n",
      "Epoch: 35 : Alpha Weight : 0.90000 | Test Acc : 96.76000 | Test Loss : 0.120 \n",
      "Epoch: 36 : Alpha Weight : 0.92500 | Test Acc : 97.03000 | Test Loss : 0.115 \n",
      "Epoch: 37 : Alpha Weight : 0.95000 | Test Acc : 96.82000 | Test Loss : 0.118 \n",
      "Epoch: 38 : Alpha Weight : 0.97500 | Test Acc : 97.08000 | Test Loss : 0.112 \n",
      "Epoch: 39 : Alpha Weight : 1.00000 | Test Acc : 96.99000 | Test Loss : 0.114 \n",
      "Epoch: 40 : Alpha Weight : 1.02500 | Test Acc : 97.00000 | Test Loss : 0.115 \n",
      "Epoch: 41 : Alpha Weight : 1.05000 | Test Acc : 97.06000 | Test Loss : 0.115 \n",
      "Epoch: 42 : Alpha Weight : 1.07500 | Test Acc : 96.98000 | Test Loss : 0.115 \n",
      "Epoch: 43 : Alpha Weight : 1.10000 | Test Acc : 96.98000 | Test Loss : 0.118 \n",
      "Epoch: 44 : Alpha Weight : 1.12500 | Test Acc : 97.08000 | Test Loss : 0.110 \n",
      "Epoch: 45 : Alpha Weight : 1.15000 | Test Acc : 97.35000 | Test Loss : 0.106 \n",
      "Epoch: 46 : Alpha Weight : 1.17500 | Test Acc : 97.28000 | Test Loss : 0.105 \n",
      "Epoch: 47 : Alpha Weight : 1.20000 | Test Acc : 97.15000 | Test Loss : 0.110 \n",
      "Epoch: 48 : Alpha Weight : 1.22500 | Test Acc : 97.17000 | Test Loss : 0.107 \n",
      "Epoch: 49 : Alpha Weight : 1.25000 | Test Acc : 97.21000 | Test Loss : 0.107 \n",
      "Epoch: 50 : Alpha Weight : 1.27500 | Test Acc : 97.35000 | Test Loss : 0.109 \n",
      "Epoch: 51 : Alpha Weight : 1.30000 | Test Acc : 97.38000 | Test Loss : 0.112 \n",
      "Epoch: 52 : Alpha Weight : 1.32500 | Test Acc : 97.48000 | Test Loss : 0.101 \n",
      "Epoch: 53 : Alpha Weight : 1.35000 | Test Acc : 97.38000 | Test Loss : 0.107 \n",
      "Epoch: 54 : Alpha Weight : 1.37500 | Test Acc : 97.35000 | Test Loss : 0.103 \n",
      "Epoch: 55 : Alpha Weight : 1.40000 | Test Acc : 97.44000 | Test Loss : 0.100 \n",
      "Epoch: 56 : Alpha Weight : 1.42500 | Test Acc : 97.42000 | Test Loss : 0.103 \n",
      "Epoch: 57 : Alpha Weight : 1.45000 | Test Acc : 97.54000 | Test Loss : 0.099 \n",
      "Epoch: 58 : Alpha Weight : 1.47500 | Test Acc : 97.49000 | Test Loss : 0.102 \n",
      "Epoch: 59 : Alpha Weight : 1.50000 | Test Acc : 97.46000 | Test Loss : 0.105 \n",
      "Epoch: 60 : Alpha Weight : 1.52500 | Test Acc : 97.43000 | Test Loss : 0.108 \n",
      "Epoch: 61 : Alpha Weight : 1.55000 | Test Acc : 97.64000 | Test Loss : 0.101 \n",
      "Epoch: 62 : Alpha Weight : 1.57500 | Test Acc : 97.64000 | Test Loss : 0.095 \n",
      "Epoch: 63 : Alpha Weight : 1.60000 | Test Acc : 97.72000 | Test Loss : 0.097 \n",
      "Epoch: 64 : Alpha Weight : 1.62500 | Test Acc : 97.64000 | Test Loss : 0.100 \n",
      "Epoch: 65 : Alpha Weight : 1.65000 | Test Acc : 97.75000 | Test Loss : 0.095 \n",
      "Epoch: 66 : Alpha Weight : 1.67500 | Test Acc : 97.70000 | Test Loss : 0.096 \n",
      "Epoch: 67 : Alpha Weight : 1.70000 | Test Acc : 97.66000 | Test Loss : 0.096 \n",
      "Epoch: 68 : Alpha Weight : 1.72500 | Test Acc : 97.76000 | Test Loss : 0.091 \n",
      "Epoch: 69 : Alpha Weight : 1.75000 | Test Acc : 97.74000 | Test Loss : 0.095 \n",
      "Epoch: 70 : Alpha Weight : 1.77500 | Test Acc : 97.70000 | Test Loss : 0.095 \n",
      "Epoch: 71 : Alpha Weight : 1.80000 | Test Acc : 97.74000 | Test Loss : 0.096 \n",
      "Epoch: 72 : Alpha Weight : 1.82500 | Test Acc : 97.82000 | Test Loss : 0.093 \n",
      "Epoch: 73 : Alpha Weight : 1.85000 | Test Acc : 97.81000 | Test Loss : 0.093 \n",
      "Epoch: 74 : Alpha Weight : 1.87500 | Test Acc : 97.80000 | Test Loss : 0.090 \n",
      "Epoch: 75 : Alpha Weight : 1.90000 | Test Acc : 97.89000 | Test Loss : 0.090 \n",
      "Epoch: 76 : Alpha Weight : 1.92500 | Test Acc : 97.93000 | Test Loss : 0.095 \n",
      "Epoch: 77 : Alpha Weight : 1.95000 | Test Acc : 97.80000 | Test Loss : 0.095 \n",
      "Epoch: 78 : Alpha Weight : 1.97500 | Test Acc : 98.00000 | Test Loss : 0.087 \n",
      "Epoch: 79 : Alpha Weight : 2.00000 | Test Acc : 97.74000 | Test Loss : 0.094 \n",
      "Epoch: 80 : Alpha Weight : 2.02500 | Test Acc : 97.99000 | Test Loss : 0.084 \n",
      "Epoch: 81 : Alpha Weight : 2.05000 | Test Acc : 97.95000 | Test Loss : 0.086 \n",
      "Epoch: 82 : Alpha Weight : 2.07500 | Test Acc : 97.75000 | Test Loss : 0.093 \n",
      "Epoch: 83 : Alpha Weight : 2.10000 | Test Acc : 98.01000 | Test Loss : 0.089 \n",
      "Epoch: 84 : Alpha Weight : 2.12500 | Test Acc : 97.86000 | Test Loss : 0.093 \n",
      "Epoch: 85 : Alpha Weight : 2.15000 | Test Acc : 97.93000 | Test Loss : 0.089 \n",
      "Epoch: 86 : Alpha Weight : 2.17500 | Test Acc : 97.85000 | Test Loss : 0.090 \n",
      "Epoch: 87 : Alpha Weight : 2.20000 | Test Acc : 98.00000 | Test Loss : 0.084 \n",
      "Epoch: 88 : Alpha Weight : 2.22500 | Test Acc : 97.92000 | Test Loss : 0.091 \n",
      "Epoch: 89 : Alpha Weight : 2.25000 | Test Acc : 97.85000 | Test Loss : 0.089 \n",
      "Epoch: 90 : Alpha Weight : 2.27500 | Test Acc : 97.88000 | Test Loss : 0.092 \n",
      "Epoch: 91 : Alpha Weight : 2.30000 | Test Acc : 97.92000 | Test Loss : 0.088 \n",
      "Epoch: 92 : Alpha Weight : 2.32500 | Test Acc : 97.93000 | Test Loss : 0.092 \n",
      "Epoch: 93 : Alpha Weight : 2.35000 | Test Acc : 97.85000 | Test Loss : 0.094 \n",
      "Epoch: 94 : Alpha Weight : 2.37500 | Test Acc : 97.88000 | Test Loss : 0.089 \n",
      "Epoch: 95 : Alpha Weight : 2.40000 | Test Acc : 98.00000 | Test Loss : 0.088 \n",
      "Epoch: 96 : Alpha Weight : 2.42500 | Test Acc : 98.01000 | Test Loss : 0.094 \n",
      "Epoch: 97 : Alpha Weight : 2.45000 | Test Acc : 98.06000 | Test Loss : 0.091 \n",
      "Epoch: 98 : Alpha Weight : 2.47500 | Test Acc : 98.09000 | Test Loss : 0.082 \n",
      "Epoch: 99 : Alpha Weight : 2.50000 | Test Acc : 98.02000 | Test Loss : 0.090 \n",
      "Epoch: 100 : Alpha Weight : 2.52500 | Test Acc : 98.01000 | Test Loss : 0.088 \n",
      "Epoch: 101 : Alpha Weight : 2.55000 | Test Acc : 98.04000 | Test Loss : 0.083 \n",
      "Epoch: 102 : Alpha Weight : 2.57500 | Test Acc : 97.97000 | Test Loss : 0.089 \n",
      "Epoch: 103 : Alpha Weight : 2.60000 | Test Acc : 98.01000 | Test Loss : 0.090 \n",
      "Epoch: 104 : Alpha Weight : 2.62500 | Test Acc : 97.93000 | Test Loss : 0.093 \n",
      "Epoch: 105 : Alpha Weight : 2.65000 | Test Acc : 98.04000 | Test Loss : 0.083 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 106 : Alpha Weight : 2.67500 | Test Acc : 98.00000 | Test Loss : 0.087 \n",
      "Epoch: 107 : Alpha Weight : 2.70000 | Test Acc : 98.09000 | Test Loss : 0.086 \n",
      "Epoch: 108 : Alpha Weight : 2.72500 | Test Acc : 98.20000 | Test Loss : 0.081 \n",
      "Epoch: 109 : Alpha Weight : 2.75000 | Test Acc : 98.11000 | Test Loss : 0.083 \n",
      "Epoch: 110 : Alpha Weight : 2.77500 | Test Acc : 97.99000 | Test Loss : 0.088 \n",
      "Epoch: 111 : Alpha Weight : 2.80000 | Test Acc : 97.95000 | Test Loss : 0.091 \n",
      "Epoch: 112 : Alpha Weight : 2.82500 | Test Acc : 98.10000 | Test Loss : 0.088 \n",
      "Epoch: 113 : Alpha Weight : 2.85000 | Test Acc : 98.13000 | Test Loss : 0.086 \n",
      "Epoch: 114 : Alpha Weight : 2.87500 | Test Acc : 98.04000 | Test Loss : 0.087 \n",
      "Epoch: 115 : Alpha Weight : 2.90000 | Test Acc : 98.10000 | Test Loss : 0.085 \n",
      "Epoch: 116 : Alpha Weight : 2.92500 | Test Acc : 98.26000 | Test Loss : 0.081 \n",
      "Epoch: 117 : Alpha Weight : 2.95000 | Test Acc : 98.10000 | Test Loss : 0.088 \n",
      "Epoch: 118 : Alpha Weight : 2.97500 | Test Acc : 98.02000 | Test Loss : 0.090 \n",
      "Epoch: 119 : Alpha Weight : 3.00000 | Test Acc : 98.27000 | Test Loss : 0.087 \n",
      "Epoch: 120 : Alpha Weight : 3.00000 | Test Acc : 98.10000 | Test Loss : 0.087 \n",
      "Epoch: 121 : Alpha Weight : 3.00000 | Test Acc : 98.14000 | Test Loss : 0.083 \n",
      "Epoch: 122 : Alpha Weight : 3.00000 | Test Acc : 98.18000 | Test Loss : 0.087 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3604c7d96b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0msemisup_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-3604c7d96b9a>\u001b[0m in \u001b[0;36msemisup_train\u001b[0;34m(model, train_loader, unlabeled_loader, test_loader)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Backpropogate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0munlabeled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('saved_models/supervised_weights'))\n",
    "\n",
    "T1 = 100\n",
    "T2 = 700\n",
    "af = 3\n",
    "\n",
    "def alpha_weight(epoch):\n",
    "    if epoch < T1:\n",
    "        return 0.0\n",
    "    elif epoch > T2:\n",
    "        return af\n",
    "    else:\n",
    "         return ((epoch-T1) / (T2-T1))*af\n",
    "        \n",
    "# Concept from : https://github.com/peimengsui/semi_supervised_mnist\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "acc_scores = []\n",
    "unlabel = []\n",
    "pseudo_label = []\n",
    "\n",
    "alpha_log = []\n",
    "test_acc_log = []\n",
    "test_loss_log = []\n",
    "def semisup_train(model, train_loader, unlabeled_loader, test_loader):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "    EPOCHS = 150\n",
    "    \n",
    "    # Instead of using current epoch we use a \"step\" variable to calculate alpha_weight\n",
    "    # This helps the model converge faster\n",
    "    step = 100 \n",
    "    \n",
    "    model.train()\n",
    "    for epoch in tqdm_notebook(range(EPOCHS)):\n",
    "        for batch_idx, x_unlabeled in enumerate(unlabeled_loader):\n",
    "            \n",
    "            \n",
    "            # Forward Pass to get the pseudo labels\n",
    "            x_unlabeled = x_unlabeled[0]\n",
    "            model.eval()\n",
    "            output_unlabeled = model(x_unlabeled)\n",
    "            _, pseudo_labeled = torch.max(output_unlabeled, 1)\n",
    "            model.train()\n",
    "            \n",
    "            \n",
    "            \"\"\" ONLY FOR VISUALIZATION\"\"\"\n",
    "            if (batch_idx < 3) and (epoch % 10 == 0):\n",
    "                unlabel.append(x_unlabeled.cpu())\n",
    "                pseudo_label.append(pseudo_labeled.cpu())\n",
    "            \"\"\" ********************** \"\"\"\n",
    "            \n",
    "            # Now calculate the unlabeled loss using the pseudo label\n",
    "            output = model(x_unlabeled)\n",
    "            unlabeled_loss = alpha_weight(step) * F.nll_loss(output, pseudo_labeled)   \n",
    "            \n",
    "            # Backpropogate\n",
    "            optimizer.zero_grad()\n",
    "            unlabeled_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            # For every 50 batches train one epoch on labeled data \n",
    "            if batch_idx % 50 == 0:\n",
    "                \n",
    "                # Normal training procedure\n",
    "                for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                    X_batch = X_batch\n",
    "                    y_batch = y_batch\n",
    "                    output = model(X_batch)\n",
    "                    labeled_loss = F.nll_loss(output, y_batch)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    labeled_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Now we increment step by 1\n",
    "                step += 1\n",
    "                \n",
    "\n",
    "        test_acc, test_loss =evaluate(model, test_loader)\n",
    "        print('Epoch: {} : Alpha Weight : {:.5f} | Test Acc : {:.5f} | Test Loss : {:.3f} '.format(epoch, alpha_weight(step), test_acc, test_loss))\n",
    "        \n",
    "        \"\"\" LOGGING VALUES \"\"\"\n",
    "        alpha_log.append(alpha_weight(step))\n",
    "        test_acc_log.append(test_acc/100)\n",
    "        test_loss_log.append(test_loss)\n",
    "        \"\"\" ************** \"\"\"\n",
    "        model.train()\n",
    "        \n",
    "semisup_train(net, train_loader, unlabeled_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
